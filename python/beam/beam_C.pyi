from __future__ import annotations
import torch
__all__ = ['cublas_gemmexrc', 'cublas_hgemmrc', 'custom_gemmrc_128x128', 'custom_gemmrc_128x256', 'cutlass_gemmrc', 'cutlass_gemmrc_spec', 'cutlass_gemmrc_splitk', 'cutlass_gemmrc_splitk_spec', 'cutlass_parallel_gemmrc_lnr']
def cublas_gemmexrc(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None:
    ...
def cublas_hgemmrc(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None:
    ...
def custom_gemmrc_128x128(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None:
    ...
def custom_gemmrc_128x256(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None:
    ...
def cutlass_gemmrc(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor) -> None:
    ...
def cutlass_gemmrc_spec(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, shape_threadblock: list[int] = [128, 128], shape_warp: list[int] = [64, 64]) -> None:
    ...
def cutlass_gemmrc_splitk(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, split_k_slices: int = 0) -> None:
    ...
def cutlass_gemmrc_splitk_spec(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, shape_threadblock: list[int] = [128, 128], shape_warp: list[int] = [64, 64], split_k_slices: int = 0) -> None:
    ...
def cutlass_parallel_gemmrc_lnr(arg0: torch.Tensor, arg1: torch.Tensor, arg2: torch.Tensor, arg3: torch.Tensor, arg4: torch.Tensor) -> None:
    ...
